{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Women_in_Machine_Learning_2020_01_28.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6-AT8XxNN9J",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation with Recurrent Neural Networks\n",
        "\n",
        "This code is a modified version of the code found here: https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfe0O9ZbjtGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p local_modules/loader\n",
        "!mkdir -p local_modules/project_tests\n",
        "!mkdir -p data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1gOuRRgy0s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('local_modules/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znCgSb1oNN9U",
        "colab_type": "code",
        "outputId": "e592ab4f-77ea-4c18-8559-f3d41d38311c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "# Import the libraries\n",
        "import collections\n",
        "import loader\n",
        "import numpy as np\n",
        "import project_tests as tests\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N11pA46Ztjcx",
        "colab_type": "code",
        "outputId": "eec21e0f-8d64-497f-bd53-cd5ff875a231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Run this cell only if you encounter problems importing libraries\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0L8iC0nNN9m",
        "colab_type": "code",
        "outputId": "7c0f711c-fc2b-4082-f7dc-aa4c2ff1fa3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load the data: English sentences + French equivalents\n",
        "english_sentences = loader.load_data('data/small_vocab_en.txt')\n",
        "french_sentences = loader.load_data('data/small_vocab_fr.txt')\n",
        "\n",
        "print('Dataset Loaded')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8of4Xf1NN9x",
        "colab_type": "code",
        "outputId": "638b3bb5-2fc2-43ca-9b6e-2eef79f2d2d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Print example sentences \n",
        "for sample_i in range(2):\n",
        "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
        "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
            "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8NqzeLINN97",
        "colab_type": "code",
        "outputId": "75dec432-c067-4c31-d22b-395abe70c245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "# Data exploration\n",
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1823250 English words.\n",
            "227 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
            "\n",
            "1961295 French words.\n",
            "355 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjPMQMCnNN-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data pre-processing - Tokenizer function\n",
        "def tokenize(x):\n",
        "    x_tk = Tokenizer(char_level = False)\n",
        "    x_tk.fit_on_texts(x)\n",
        "    return x_tk.texts_to_sequences(x), x_tk\n",
        "\n",
        "tests.test_tokenize(tokenize)\n",
        "\n",
        "# Tokenize Example output\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bDTbHwaNN-P",
        "colab_type": "code",
        "outputId": "98e04e74-c125-457a-c006-298fb8951755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "# Data pre-processing - Padding function\n",
        "def pad(x, length=None):\n",
        "\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
        "\n",
        "tests.test_pad(pad)\n",
        "\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 2 4 5 6 7 1 8 9]\n",
            "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
            "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
            "Sequence 3 in x\n",
            "  Input:  [18 19  3 20 21]\n",
            "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzK61Uf4NN-Z",
        "colab_type": "code",
        "outputId": "300a1eea-8584-47da-910b-c3b5a9a5d692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Data pre-processing - Pipeline including padding and tokenization\n",
        "def preprocess(x, y):\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 15\n",
            "Max French sentence length: 21\n",
            "English vocabulary size: 200\n",
            "French vocabulary size: 345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GBxPuLDNN-g",
        "colab_type": "code",
        "outputId": "dc1f3f3d-d385-4bfc-ff4d-bcf51477aa0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Logits to text function: Mapping of logits to French words\n",
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`logits_to_text` function loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZkGVzktvSNh",
        "colab_type": "code",
        "outputId": "587a3662-f383-45c9-8276-40a8a436eaeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        }
      },
      "source": [
        "# A simple recurrent neural network (RNN) model\n",
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    input_seq = Input(input_shape[1:])\n",
        "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
        "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
        "    model = Model(input_seq, Activation('softmax')(logits))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tests.test_simple_model(simple_model)\n",
        "\n",
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "# Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "110288/110288 [==============================] - 49s 442us/step - loss: 3.5049 - acc: 0.4007 - val_loss: 2.6194 - val_acc: 0.4556\n",
            "Epoch 2/10\n",
            "110288/110288 [==============================] - 47s 427us/step - loss: 2.5078 - acc: 0.4640 - val_loss: 2.3959 - val_acc: 0.4775\n",
            "Epoch 3/10\n",
            "110288/110288 [==============================] - 47s 429us/step - loss: 2.2733 - acc: 0.4962 - val_loss: 2.1417 - val_acc: 0.5223\n",
            "Epoch 4/10\n",
            "110288/110288 [==============================] - 47s 429us/step - loss: 2.0173 - acc: 0.5373 - val_loss: 1.8869 - val_acc: 0.5654\n",
            "Epoch 5/10\n",
            "110288/110288 [==============================] - 47s 427us/step - loss: 1.7973 - acc: 0.5744 - val_loss: 1.7231 - val_acc: 0.5827\n",
            "Epoch 6/10\n",
            "110288/110288 [==============================] - 47s 428us/step - loss: 1.6759 - acc: 0.5825 - val_loss: 1.6331 - val_acc: 0.5868\n",
            "Epoch 7/10\n",
            "110288/110288 [==============================] - 47s 423us/step - loss: 1.6011 - acc: 0.5908 - val_loss: 1.5700 - val_acc: 0.5974\n",
            "Epoch 8/10\n",
            "110288/110288 [==============================] - 47s 424us/step - loss: 1.5454 - acc: 0.5996 - val_loss: 1.5204 - val_acc: 0.6044\n",
            "Epoch 9/10\n",
            "110288/110288 [==============================] - 47s 426us/step - loss: 1.4996 - acc: 0.6073 - val_loss: 1.4783 - val_acc: 0.6120\n",
            "Epoch 10/10\n",
            "110288/110288 [==============================] - 47s 426us/step - loss: 1.4593 - acc: 0.6134 - val_loss: 1.4401 - val_acc: 0.6154\n",
            "new jersey est parfois parfois en en mais il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHJSDQBtNN-m",
        "colab_type": "code",
        "outputId": "dae81f69-6223-487e-ddb6-85e78fac3caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "# The Encoder-Decoder model \n",
        "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "  \n",
        "    learning_rate = 1e-3\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False)) # Encoder\n",
        "    model.add(RepeatVector(output_sequence_length)) # Context vector\n",
        "    model.add(GRU(128, return_sequences = True)) # Decoder\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "tests.test_encdec_model(encdec_model)\n",
        "tmp_x = pad(preproc_english_sentences)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\n",
        "\n",
        "encodeco_model = encdec_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "encodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
        "print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/20\n",
            "110288/110288 [==============================] - 99s 902us/step - loss: 2.9881 - acc: 0.4424 - val_loss: 2.4311 - val_acc: 0.4865\n",
            "Epoch 2/20\n",
            "110288/110288 [==============================] - 97s 882us/step - loss: 2.2779 - acc: 0.5008 - val_loss: 2.1252 - val_acc: 0.5040\n",
            "Epoch 3/20\n",
            "110288/110288 [==============================] - 97s 879us/step - loss: 2.0423 - acc: 0.5082 - val_loss: 1.9682 - val_acc: 0.5213\n",
            "Epoch 4/20\n",
            "110288/110288 [==============================] - 97s 879us/step - loss: 1.9021 - acc: 0.5281 - val_loss: 1.8249 - val_acc: 0.5404\n",
            "Epoch 5/20\n",
            "110288/110288 [==============================] - 97s 883us/step - loss: 1.7659 - acc: 0.5478 - val_loss: 1.7035 - val_acc: 0.5586\n",
            "Epoch 6/20\n",
            "110288/110288 [==============================] - 97s 877us/step - loss: 1.6635 - acc: 0.5698 - val_loss: 1.6184 - val_acc: 0.5759\n",
            "Epoch 7/20\n",
            "110288/110288 [==============================] - 97s 882us/step - loss: 1.5870 - acc: 0.5796 - val_loss: 1.5517 - val_acc: 0.5854\n",
            "Epoch 8/20\n",
            "110288/110288 [==============================] - 98s 887us/step - loss: 1.5239 - acc: 0.5892 - val_loss: 1.4984 - val_acc: 0.5949\n",
            "Epoch 9/20\n",
            "110288/110288 [==============================] - 97s 883us/step - loss: 1.4689 - acc: 0.6007 - val_loss: 1.4397 - val_acc: 0.6057\n",
            "Epoch 10/20\n",
            "110288/110288 [==============================] - 97s 884us/step - loss: 1.4228 - acc: 0.6090 - val_loss: 1.4001 - val_acc: 0.6121\n",
            "Epoch 11/20\n",
            "110288/110288 [==============================] - 98s 887us/step - loss: 1.3813 - acc: 0.6166 - val_loss: 1.3614 - val_acc: 0.6201\n",
            "Epoch 12/20\n",
            "110288/110288 [==============================] - 97s 884us/step - loss: 1.3518 - acc: 0.6217 - val_loss: 1.3386 - val_acc: 0.6259\n",
            "Epoch 13/20\n",
            "110288/110288 [==============================] - 98s 886us/step - loss: 1.3286 - acc: 0.6267 - val_loss: 1.3156 - val_acc: 0.6291\n",
            "Epoch 14/20\n",
            "110288/110288 [==============================] - 98s 885us/step - loss: 1.3097 - acc: 0.6304 - val_loss: 1.2996 - val_acc: 0.6319\n",
            "Epoch 15/20\n",
            "110288/110288 [==============================] - 97s 882us/step - loss: 1.2946 - acc: 0.6331 - val_loss: 1.2875 - val_acc: 0.6336\n",
            "Epoch 16/20\n",
            "110288/110288 [==============================] - 97s 881us/step - loss: 1.2795 - acc: 0.6359 - val_loss: 1.2696 - val_acc: 0.6383\n",
            "Epoch 17/20\n",
            "110288/110288 [==============================] - 97s 883us/step - loss: 1.2665 - acc: 0.6385 - val_loss: 1.2579 - val_acc: 0.6391\n",
            "Epoch 18/20\n",
            "110288/110288 [==============================] - 98s 885us/step - loss: 1.2525 - acc: 0.6423 - val_loss: 1.2527 - val_acc: 0.6407\n",
            "Epoch 19/20\n",
            "110288/110288 [==============================] - 97s 881us/step - loss: 1.2353 - acc: 0.6440 - val_loss: 1.2233 - val_acc: 0.6461\n",
            "Epoch 20/20\n",
            "110288/110288 [==============================] - 97s 880us/step - loss: 1.2190 - acc: 0.6464 - val_loss: 1.2158 - val_acc: 0.6464\n",
            "new jersey est parfois agréable en mois mais il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi3zcLSoqwEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}